{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "702a8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c7e6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o diretório dos documentos\n",
    "data_dir = \"rag_project/data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b55e391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os documentos\n",
    "def load_documents(directory):\n",
    "    \"\"\"\n",
    "    Carrega todos os documentos de texto de um diretório.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Caminho para o diretório contendo os documentos.\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de documentos carregados.\n",
    "    \"\"\"\n",
    "    print(f\"Carregando documentos do diretório: {directory}\")\n",
    "    \n",
    "    # Usar o DirectoryLoader do LangChain para carregar todos os arquivos .txt\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    \n",
    "    documents = loader.load()\n",
    "    print(f\"Carregados {len(documents)} documentos.\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Segmentar os documentos em chunks menores\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Divide os documentos em chunks menores para processamento.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): Lista de documentos a serem divididos.\n",
    "        chunk_size (int): Tamanho de cada chunk em caracteres.\n",
    "        chunk_overlap (int): Sobreposição entre chunks consecutivos.\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de chunks de documentos.\n",
    "    \"\"\"\n",
    "    print(f\"Segmentando documentos em chunks de {chunk_size} caracteres com sobreposição de {chunk_overlap}.\")\n",
    "    \n",
    "    # Usar o RecursiveCharacterTextSplitter do LangChain para dividir os documentos\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Documentos segmentados em {len(chunks)} chunks.\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "303df435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando documentos do diretório: rag_project/data/raw\n",
      "Carregados 9 documentos.\n",
      "Segmentando documentos em chunks de 1000 caracteres com sobreposição de 200.\n",
      "Documentos segmentados em 23 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Executar o carregamento e segmentação\n",
    "documents = load_documents(data_dir)\n",
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0723e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo de embedding da Vertex AI\n",
    "def initialize_embeddings():\n",
    "    \"\"\"\n",
    "    Inicializa o modelo de embedding da Vertex AI.\n",
    "    \n",
    "    Returns:\n",
    "        VertexAIEmbeddings: Instância do modelo de embedding.\n",
    "    \"\"\"\n",
    "    print(\"Inicializando o modelo de embedding da Vertex AI...\")\n",
    "    \n",
    "    embeddings = VertexAIEmbeddings(\n",
    "      model_name=\"text-embedding-004\",\n",
    "      project=\"ccdsiaa\",\n",
    "      location=\"us-central1\"\n",
    "    )\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da050d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o índice FAISS\n",
    "def create_vector_index(chunks, embeddings):\n",
    "    \"\"\"\n",
    "    Cria um índice vetorial FAISS a partir dos chunks de documentos.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): Lista de chunks de documentos.\n",
    "        embeddings: Modelo de embedding a ser utilizado.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS: Índice vetorial FAISS.\n",
    "    \"\"\"\n",
    "    print(\"Criando índice vetorial FAISS...\")\n",
    "    \n",
    "    # Criar o índice FAISS\n",
    "    vector_index = FAISS.from_documents(chunks, embeddings)\n",
    "    \n",
    "    print(\"Índice vetorial criado com sucesso.\")\n",
    "    \n",
    "    return vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52235cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o índice FAISS\n",
    "def save_vector_index(vector_index, path):\n",
    "    \"\"\"\n",
    "    Salva o índice vetorial FAISS em disco.\n",
    "    \n",
    "    Args:\n",
    "        vector_index: Índice vetorial FAISS.\n",
    "        path (str): Caminho para salvar o índice.\n",
    "    \"\"\"\n",
    "    print(f\"Salvando índice vetorial em {path}...\")\n",
    "    \n",
    "    # Criar o diretório se não existir\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Salvar o índice\n",
    "    vector_index.save_local(path)\n",
    "    \n",
    "    print(\"Índice vetorial salvo com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0e4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=kWmZkFGHvjq2WzK0RHQgiZmQCLtv71&access_type=offline&code_challenge=mGkPS7cvlGBglBqxEipaJiV4ZxPuxJNLZpdB7jj4ce0&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\marri\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"ccdsiaa\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0375769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando o modelo de embedding da Vertex AI...\n",
      "Criando índice vetorial FAISS...\n",
      "Índice vetorial criado com sucesso.\n",
      "Salvando índice vetorial em rag_project/models/faiss_index...\n",
      "Índice vetorial salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Executar a geração de embeddings e indexação\n",
    "embeddings = initialize_embeddings()\n",
    "vector_index = create_vector_index(chunks, embeddings)\n",
    "save_vector_index(vector_index, \"rag_project/models/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbce9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o índice FAISS (se necessário)\n",
    "def load_vector_index(path, embeddings):\n",
    "    \"\"\"\n",
    "    Carrega um índice vetorial FAISS do disco.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Caminho do índice.\n",
    "        embeddings: Modelo de embedding a ser utilizado.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS: Índice vetorial FAISS.\n",
    "    \"\"\"\n",
    "    print(f\"Carregando índice vetorial de {path}...\")\n",
    "    \n",
    "    # Carregar o índice\n",
    "    vector_index = FAISS.load_local(path, embeddings)\n",
    "    \n",
    "    print(\"Índice vetorial carregado com sucesso.\")\n",
    "    \n",
    "    return vector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o LLM da Vertex AI\n",
    "def initialize_llm():\n",
    "    \"\"\"\n",
    "    Inicializa o modelo de linguagem da Vertex AI.\n",
    "    \n",
    "    Returns:\n",
    "        VertexAI: Instância do modelo de linguagem.\n",
    "    \"\"\"\n",
    "    print(\"Inicializando o modelo de linguagem da Vertex AI...\")\n",
    "    \n",
    "    llm = VertexAI(\n",
    "        model_name=\"gemini-2.5-flash-preview-05-20\",\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.2,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        project=\"ccdsiaa\",\n",
    "        location=\"us-central1\"\n",
    "    )\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da61b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o template de prompt\n",
    "def create_prompt_template():\n",
    "    \"\"\"\n",
    "    Cria um template de prompt para o LLM com uma persona proativa.\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: Template de prompt.\n",
    "    \"\"\"\n",
    "    template = \"\"\"\n",
    "    Aja como o \"Guia UTF\", o assistente virtual oficial da UTFPR-CM (Universidade Tecnológica Federal do Paraná campus Campo Mourão). Você é um especialista nos regulamentos, datas e contatos da universidade. Seu objetivo é fornecer respostas claras, precisas e úteis para os estudantes.\n",
    "\n",
    "    **Suas regras de ouro são:**\n",
    "    1.  **Baseie-se nos Fatos:** Responda UNICAMENTE com base no CONTEXTO fornecido. Não invente informações, datas ou processos.\n",
    "    2.  **Seja Honesto:** Se a resposta não estiver no CONTEXTO, diga explicitamente: \"Não encontrei essa informação nos documentos disponíveis.\"\n",
    "    3.  **Seja Útil Mesmo Sem a Resposta:** Após admitir que não sabe, sempre tente ajudar. Sugira um próximo passo, como: \"Para informações sobre isso, o ideal é contatar diretamente o DERAC (Departamento de Registros Acadêmicos) no Bloco X\" ou \"Recomendo que você converse com o coordenador do seu curso para um conselho mais personalizado.\"\n",
    "    4.  **Tom e Estilo:** Mantenha um tom amigável, encorajador e respeitoso. Organize a informação em listas ou parágrafos curtos para facilitar a leitura.\n",
    "\n",
    "    CONTEXTO:\n",
    "    {context}\n",
    "    \n",
    "    PERGUNTA DO ALUNO:\n",
    "    {question}\n",
    "    \n",
    "    SUA RESPOSTA (COMO GUIA UTF):\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4dc72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a função de recuperação e geração\n",
    "def create_rag_chain(vector_index, llm, prompt_template):\n",
    "    \"\"\"\n",
    "    Cria uma cadeia de recuperação e geração (RAG).\n",
    "    \n",
    "    Args:\n",
    "        vector_index: Índice vetorial FAISS.\n",
    "        llm: Modelo de linguagem.\n",
    "        prompt_template: Template de prompt.\n",
    "        \n",
    "    Returns:\n",
    "        RetrievalQA: Cadeia de recuperação e geração.\n",
    "    \"\"\"\n",
    "    print(\"Criando cadeia RAG...\")\n",
    "    \n",
    "    # Criar o retriever\n",
    "    retriever = vector_index.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}  # Recuperar os 5 chunks mais relevantes\n",
    "    )\n",
    "    \n",
    "    # Criar a cadeia RAG\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # \"stuff\" combina todos os documentos em um único prompt\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template}\n",
    "    )\n",
    "    \n",
    "    print(\"Cadeia RAG criada com sucesso.\")\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ead92920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal para responder perguntas\n",
    "def answer_question(question, rag_chain):\n",
    "    \"\"\"\n",
    "    Responde a uma pergunta utilizando a cadeia RAG.\n",
    "    \n",
    "    Args:\n",
    "        question (str): Pergunta do usuário.\n",
    "        rag_chain: Cadeia RAG.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Resposta e documentos fonte.\n",
    "    \"\"\"\n",
    "    print(f\"Processando pergunta: {question}\")\n",
    "    \n",
    "    # Executar a cadeia RAG\n",
    "    result = rag_chain({\"query\": question})\n",
    "    \n",
    "    return {\n",
    "        \"pergunta\": question,\n",
    "        \"resposta\": result[\"result\"],\n",
    "        \"documentos_fonte\": result[\"source_documents\"]\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
